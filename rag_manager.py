"""
Modul pentru gestionarea RAG-ului cu vector store izolat per tenant.
FoloseÈ™te Ollama embeddings pentru semantic search.
"""
import os
import json
import pickle
import numpy as np
from typing import List, Dict, Optional, Tuple
from ollama import Client
import hashlib

# Conectare la Ollama pentru embeddings
OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'localhost:11434')
ollama = Client(host=OLLAMA_HOST)

# Model pentru embeddings (foloseÈ™te acelaÈ™i model ca pentru chat sau unul specializat)
EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'nomic-embed-text')  # Model optimizat pentru embeddings

# Director pentru stocarea vector stores per tenant
VECTOR_STORE_DIR = "vector_stores"

def get_tenant_vector_store_path(tenant_id: str) -> str:
    """ReturneazÄƒ calea cÄƒtre vector store-ul unui tenant"""
    return os.path.join(VECTOR_STORE_DIR, tenant_id)

def get_embedding(text: str) -> List[float]:
    """
    ObÈ›ine embedding-ul pentru un text folosind Ollama.
    DacÄƒ modelul de embeddings nu este disponibil, foloseÈ™te un fallback.
    """
    try:
        # ÃncearcÄƒ sÄƒ foloseascÄƒ modelul de embeddings
        response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)
        if response and 'embedding' in response:
            return response['embedding']
    except Exception as e:
        print(f"âš ï¸ Eroare la obÈ›inerea embedding-ului cu {EMBEDDING_MODEL}: {e}")
        print("ğŸ’¡ Folosind fallback: hash-based similarity")
    
    # Fallback: foloseÈ™te hash pentru simplitate (nu este semantic, dar funcÈ›ioneazÄƒ)
    # Ãn producÈ›ie, ar trebui sÄƒ foloseÈ™ti un model de embeddings real
    hash_obj = hashlib.md5(text.encode())
    # Convertim hash-ul Ã®ntr-un vector de dimensiune fixÄƒ (128)
    hash_bytes = hash_obj.digest()
    vector = [float(b) / 255.0 for b in hash_bytes] * (128 // len(hash_bytes) + 1)
    return vector[:128]

def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """CalculeazÄƒ similaritatea cosinus Ã®ntre doi vectori"""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return float(dot_product / (norm1 * norm2))

class TenantRAGStore:
    """Stocare RAG izolatÄƒ per tenant cu vector store"""
    
    def __init__(self, tenant_id: str):
        self.tenant_id = tenant_id
        self.store_path = get_tenant_vector_store_path(tenant_id)
        self.embeddings_file = os.path.join(self.store_path, "embeddings.pkl")
        self.metadata_file = os.path.join(self.store_path, "metadata.json")
        
        # ÃncarcÄƒ datele existente
        self.embeddings: List[List[float]] = []
        self.metadata: List[Dict] = []  # [{filename, content, chunk_index}, ...]
        
        self._load_store()
    
    def _load_store(self):
        """ÃncarcÄƒ vector store-ul din disk"""
        os.makedirs(self.store_path, exist_ok=True)
        
        if os.path.exists(self.embeddings_file) and os.path.exists(self.metadata_file):
            try:
                with open(self.embeddings_file, 'rb') as f:
                    self.embeddings = pickle.load(f)
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                print(f"âœ… Vector store Ã®ncÄƒrcat pentru tenant {self.tenant_id}: {len(self.embeddings)} documente")
            except Exception as e:
                print(f"âš ï¸ Eroare la Ã®ncÄƒrcarea vector store pentru {self.tenant_id}: {e}")
                self.embeddings = []
                self.metadata = []
    
    def _save_store(self):
        """SalveazÄƒ vector store-ul pe disk"""
        os.makedirs(self.store_path, exist_ok=True)
        
        try:
            with open(self.embeddings_file, 'wb') as f:
                pickle.dump(self.embeddings, f)
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            print(f"âœ… Vector store salvat pentru tenant {self.tenant_id}")
        except Exception as e:
            print(f"âŒ Eroare la salvarea vector store pentru {self.tenant_id}: {e}")
    
    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """Ãmparte textul Ã®n chunk-uri pentru o indexare mai bunÄƒ"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - overlap  # Overlap pentru context
        
        return chunks
    
    def add_document(self, filename: str, content: str):
        """
        AdaugÄƒ un document Ã®n vector store.
        DacÄƒ documentul existÄƒ deja, Ã®l Ã®nlocuieÈ™te.
        """
        # È˜terge documentul existent dacÄƒ existÄƒ
        self.remove_document(filename)
        
        # Ãmparte Ã®n chunk-uri
        chunks = self._chunk_text(content)
        
        # GenereazÄƒ embeddings pentru fiecare chunk
        for chunk_idx, chunk in enumerate(chunks):
            embedding = get_embedding(chunk)
            self.embeddings.append(embedding)
            self.metadata.append({
                "filename": filename,
                "content": chunk,
                "chunk_index": chunk_idx,
                "total_chunks": len(chunks)
            })
        
        self._save_store()
        print(f"âœ… Document {filename} adÄƒugat Ã®n vector store pentru tenant {self.tenant_id} ({len(chunks)} chunk-uri)")
    
    def remove_document(self, filename: str):
        """È˜terge un document din vector store"""
        initial_count = len(self.embeddings)
        
        # GÄƒseÈ™te toate chunk-urile pentru acest document
        indices_to_remove = [
            i for i, meta in enumerate(self.metadata)
            if meta.get("filename") == filename
        ]
        
        # È˜terge Ã®n ordine inversÄƒ pentru a nu afecta indicii
        for idx in reversed(indices_to_remove):
            self.embeddings.pop(idx)
            self.metadata.pop(idx)
        
        if len(self.embeddings) < initial_count:
            self._save_store()
            print(f"âœ… Document {filename} È™ters din vector store pentru tenant {self.tenant_id}")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        CautÄƒ Ã®n vector store È™i returneazÄƒ top_k rezultate relevante.
        ReturneazÄƒ: [{filename, content, score}, ...]
        """
        if not self.embeddings:
            return []
        
        # GenereazÄƒ embedding pentru query
        query_embedding = get_embedding(query)
        
        # CalculeazÄƒ similaritÄƒÈ›i
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            similarity = cosine_similarity(query_embedding, doc_embedding)
            similarities.append((i, similarity))
        
        # SorteazÄƒ dupÄƒ similaritate
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # ReturneazÄƒ top_k rezultate
        results = []
        seen_files = set()  # Pentru a evita duplicatele
        
        for idx, score in similarities[:top_k * 3]:  # LuÄƒm mai multe pentru a filtra duplicatele
            meta = self.metadata[idx]
            filename = meta.get("filename", "unknown")
            
            # AdaugÄƒ doar dacÄƒ nu am vÄƒzut deja acest fiÈ™ier sau dacÄƒ avem puÈ›ine rezultate
            if filename not in seen_files or len(results) < top_k:
                results.append({
                    "filename": filename,
                    "content": meta.get("content", ""),
                    "score": score,
                    "chunk_index": meta.get("chunk_index", 0),
                    "total_chunks": meta.get("total_chunks", 1)
                })
                seen_files.add(filename)
            
            if len(results) >= top_k:
                break
        
        return results
    
    def get_all_documents(self) -> List[Dict]:
        """ReturneazÄƒ toate documentele (fÄƒrÄƒ duplicate)"""
        seen = set()
        documents = []
        
        for meta in self.metadata:
            filename = meta.get("filename", "unknown")
            if filename not in seen:
                # ColecteazÄƒ toate chunk-urile pentru acest document
                chunks = [
                    m.get("content", "")
                    for m in self.metadata
                    if m.get("filename") == filename
                ]
                documents.append({
                    "filename": filename,
                    "content": "\n\n".join(chunks)  # Reconstituie documentul complet
                })
                seen.add(filename)
        
        return documents
    
    def clear(self):
        """È˜terge tot vector store-ul"""
        self.embeddings = []
        self.metadata = []
        self._save_store()
        print(f"âœ… Vector store È™ters pentru tenant {self.tenant_id}")

# Cache pentru store-uri per tenant
_tenant_stores: Dict[str, TenantRAGStore] = {}

def get_tenant_rag_store(tenant_id: str) -> TenantRAGStore:
    """ObÈ›ine sau creeazÄƒ vector store-ul pentru un tenant"""
    if tenant_id not in _tenant_stores:
        _tenant_stores[tenant_id] = TenantRAGStore(tenant_id)
    return _tenant_stores[tenant_id]

